{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency\n",
    "#### Group Member Candidate Number: ZWWD7, BXPM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pandas.read_csv('inputs.csv', header=None)\n",
    "outputs = pandas.read_csv('outputs.csv', header=None)\n",
    "transactions = pandas.read_csv('transactions.csv', header=None)\n",
    "inputs.columns = [\"id\", \"tx_id\", \"sig_id\", \"output_id\"]\n",
    "outputs.columns = [\"id\", \"tx_id\", \"pk_id\", \"value\"]\n",
    "transactions.columns = ['id', \"block_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Basic statistic\n",
    "### 1.1 Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_1_1():\n",
    "    print (\"------------------- 1.1 -------------------\")\n",
    "    print (\"How many transactions were there in total?\")\n",
    "    print (len(transactions['id'].unique()))\n",
    "    print (\"Of these, how many transactions had one input and two outputs?\")\n",
    "    count_outputs = outputs['tx_id'].value_counts()\n",
    "    count_inputs = inputs['tx_id'].value_counts()\n",
    "    count_outputs_1 = count_outputs[count_outputs == 1]\n",
    "    count_outputs_2 = count_outputs[count_outputs == 2]\n",
    "    count_inputs_1 = count_inputs[count_inputs == 1]\n",
    "    print (len(count_outputs_2[count_outputs_2.index.isin(count_inputs_1.index)]))\n",
    "    print (\"How many transactions had one input and one output?\")\n",
    "    print (len(count_outputs_1[count_outputs_1.index.isin(count_inputs_1.index)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 1.1 -------------------\n",
      "How many transactions were there in total?\n",
      "216626\n",
      "Of these, how many transactions had one input and two outputs?\n",
      "44898\n",
      "How many transactions had one input and one output?\n",
      "160780\n"
     ]
    }
   ],
   "source": [
    "answer_1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 UTXOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_1_2():\n",
    "    print (\"------------------- 1.2 -------------------\")\n",
    "    temp_outputs = outputs[['id','pk_id','value']]\n",
    "    temp_inputs = inputs['output_id']\n",
    "    UTXOs = temp_outputs[~temp_outputs['id'].isin(temp_inputs)].set_index('id')\n",
    "    print (\"How many UTXOs exist, as of the last block of the dataset?\")\n",
    "    print (len(UTXOs))\n",
    "    print (\"Which UTXO has the highest associated value?\")\n",
    "    print (\"output id:\", UTXOs.value.idxmax())\n",
    "    print (\"value:\", UTXOs.value.max())\n",
    "    return UTXOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 1.2 -------------------\n",
      "How many UTXOs exist, as of the last block of the dataset?\n",
      "71906\n",
      "Which UTXO has the highest associated value?\n",
      "output id: 170430\n",
      "value: 9000000000000\n"
     ]
    }
   ],
   "source": [
    "UTXOs = answer_1_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Public keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_1_3():\n",
    "    print (\"------------------- 1.3 -------------------\")\n",
    "    print (\"How many distinct public keys were used across all blocks in the dataset?\")\n",
    "    #pk_ids = pandas.concat([outputs['pk_id'], tags['pk_id']], axis=0, ignore_index=True).unique()\n",
    "    print (len(outputs['pk_id'].unique()))\n",
    "    print (\"Which public key received the highest number of bitcoins, and how many bitcoins has it received?\")\n",
    "    temp = outputs\n",
    "    temp = temp.groupby('pk_id')['value'].sum()\n",
    "    print(\"pk_id:\", temp.idxmax(), \"value:\", temp.max())\n",
    "    print (\"Which public key acted as an output the most number of times, and how many times did it act as output?\")\n",
    "    count_pk = outputs['pk_id'].value_counts()\n",
    "    print (\"pk_id:\", count_pk.idxmax(), \"value:\", count_pk.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 1.3 -------------------\n",
      "How many distinct public keys were used across all blocks in the dataset?\n",
      "174702\n",
      "Which public key received the highest number of bitcoins, and how many bitcoins has it received?\n",
      "pk_id: 148105 value: 27375023000000\n",
      "Which public key acted as an output the most number of times, and how many times did it act as output?\n",
      "pk_id: 148105 value: 5498\n"
     ]
    }
   ],
   "source": [
    "answer_1_3()\n",
    "# outputs.loc[outputs['pk_id']==148105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Invalid transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_1_4():\n",
    "    print (\"------------------- 1.4 -------------------\")\n",
    "    ## double spend\n",
    "    temp_spend = inputs\n",
    "    temp_spend = temp_spend.loc[(temp_spend['output_id'].duplicated()) & (temp_spend['output_id'] != -1)]\n",
    "    i = 0\n",
    "    for _, row in temp_spend.iterrows():\n",
    "        i += 1\n",
    "        print (str(i)+\". double spend:\")\n",
    "        print (\"id:\", row.id, \"tx_id:\", row.tx_id, \"sig_id:\", row.sig_id, \"output_id:\", row.output_id)\n",
    "        print (\"spent in:\")\n",
    "        s = inputs.loc[(inputs['output_id'] == row.output_id) & (inputs['id'] != row.id)]\n",
    "        #print (\"id:\", s.id, \"tx_id:\", s.tx_id, \"sig_id:\", s.sig_id, \"output_id:\", s.output_id)\n",
    "        row = s.reset_index().loc[0]\n",
    "        print (\"id:\", row.id, \"tx_id:\", row.tx_id, \"sig_id:\", row.sig_id, \"output_id:\", row.output_id)\n",
    "        print (\"\")\n",
    "        \n",
    "    ## ouput > input\n",
    "    temp_input = inputs\n",
    "    temp_output = outputs\n",
    "    money_in = pandas.merge(temp_input[['tx_id','output_id']],temp_output[['id','value']],left_on='output_id',right_on='id')[['tx_id','value']]\n",
    "    money_in = money_in.groupby('tx_id')['value'].sum()\n",
    "    dict_money_in = {'tx_id':money_in.index,'money_in':money_in.values}\n",
    "    money_in = pandas.DataFrame(dict_money_in)\n",
    "    money_out = temp_output.groupby('tx_id')['value'].sum()\n",
    "    dict_money_out = {'tx_id':money_out.index,'money_out':money_out.values}\n",
    "    money_out = pandas.DataFrame(dict_money_out)\n",
    "    transactions = pandas.merge(money_in[['tx_id','money_in']], money_out[['tx_id','money_out']], on='tx_id')\n",
    "    invalid_tx = transactions.loc[(transactions['money_in']<transactions['money_out'])]\n",
    "    for _, row in invalid_tx.iterrows():\n",
    "        i += 1\n",
    "        print (str(i)+\". output > input:\")\n",
    "        print (\"tx_id:\", row.tx_id, \"money_in:\", row.money_in, \"money_out:\", row.money_out)\n",
    "        print (\"money_in:\")\n",
    "        j = 0\n",
    "        for _, row1 in inputs.loc[inputs['tx_id']==row.tx_id].iterrows():\n",
    "            for _, row2 in outputs.loc[outputs['id']==row1.output_id].iterrows():\n",
    "                j += 1\n",
    "                print (str(j)+\") input_id:\", row1.tx_id, \"UTXO_id:\", row1.output_id, \"owner_id:\", row1.sig_id, \"value:\",row2.value)\n",
    "        print (\"money_out:\")\n",
    "        j = 0\n",
    "        for _, row1 in outputs.loc[outputs['tx_id']==row.tx_id].iterrows():\n",
    "            j += 1\n",
    "            print (str(j)+\") output_id:\", row1.id, \"payee_id:\", row1.pk_id, \"value:\", row1.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 1.4 -------------------\n",
      "1. double spend:\n",
      "id: 12820 tx_id: 12152 sig_id: 7941 output_id: 7998\n",
      "spent in:\n",
      "id: 8666 tx_id: 8231 sig_id: 7941 output_id: 7998\n",
      "\n",
      "2. double spend:\n",
      "id: 33114 tx_id: 30446 sig_id: 21807 output_id: 21928\n",
      "spent in:\n",
      "id: 33113 tx_id: 30446 sig_id: 21807 output_id: 21928\n",
      "\n",
      "3. double spend:\n",
      "id: 76750 tx_id: 61845 sig_id: 138980 output_id: 65403\n",
      "spent in:\n",
      "id: 76747 tx_id: 61843 sig_id: 138980 output_id: 65403\n",
      "\n",
      "4. double spend:\n",
      "id: 279609 tx_id: 207365 sig_id: 163625 output_id: 249860\n",
      "spent in:\n",
      "id: 275614 tx_id: 204751 sig_id: 163625 output_id: 249860\n",
      "\n",
      "5. output > input:\n",
      "tx_id: 100929 money_in: 5000000000 money_out: 5000000010\n",
      "money_in:\n",
      "1) input_id: 100929 UTXO_id: 37553 owner_id: 37334 value: 5000000000\n",
      "money_out:\n",
      "1) output_id: 118119 payee_id: 60205 value: 2500000000\n",
      "2) output_id: 118120 payee_id: 142642 value: 2500000010\n"
     ]
    }
   ],
   "source": [
    "answer_1_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Clustering\n",
    "### generate cluster dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_clusters():\n",
    "    max_cluster_id = 0\n",
    "    # init a new data frame to store clusters, columns = ['pk_id','cluster_id']\n",
    "    clusters = pandas.DataFrame()\n",
    "    # set pk_id columns to all unique sig_id in inputs\n",
    "    clusters['pk_id'] = inputs['sig_id'].unique()\n",
    "    # set index to pk_id\n",
    "    clusters = clusters.set_index('pk_id')\n",
    "    # init all cluster_id as 0\n",
    "    clusters['cluster_id'] = 0\n",
    "\n",
    "    # recursion funtion to find all related publick keys with input pk_id, store and return them in pk_id_list\n",
    "    def get_all_related_pk_id(pk_id,pk_id_list):\n",
    "        # get all transactions containing the input pk_id\n",
    "        tx_id_array = inputs.loc[inputs['sig_id']==pk_id,'tx_id'].unique()\n",
    "        # traverse transactions\n",
    "        for tx_id_related in tx_id_array:\n",
    "            # get all public keys in this transaction\n",
    "            # the public key is related to the input pk_id, thus they should be clustered in same group\n",
    "            pk_id_array = inputs.loc[inputs['tx_id']==tx_id_related,'sig_id'].unique()\n",
    "            # traverse public keys\n",
    "            for pk_id_related in pk_id_array:\n",
    "                # if related public key has not yet been in the related public key list,\n",
    "                # add it in and find public keys related to this one recursively\n",
    "                if pk_id_related not in pk_id_list:\n",
    "                    pk_id_list += [pk_id_related]\n",
    "                    if pk_id_related == pk_id:\n",
    "                        continue\n",
    "                    pk_id_list = get_all_related_pk_id(pk_id_related,pk_id_list)\n",
    "        return pk_id_list\n",
    "    \n",
    "#     # traverse function to find all related publick keys with input pk_id, store and return them in pk_id_list\n",
    "#     def get_all_related_pk_id(pk_id,pk_id_list):\n",
    "#         pk_id_list = [pk_id]\n",
    "#         temp_pk_id_list = [pk_id]\n",
    "#         while True:\n",
    "#             tx_id_array = inputs.loc[inputs['sig_id'].isin(temp_pk_id_list),'tx_id'].unique()\n",
    "#             pk_id_array = inputs.loc[inputs['tx_id'].isin(tx_id_array),'sig_id'].unique()\n",
    "#             temp_pk_id_list = [x for x in pk_id_array if x not in pk_id_list]\n",
    "#             pk_id_list += temp_pk_id_list\n",
    "#             if len(temp_pk_id_list) != 0:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 return pk_id_list\n",
    "            \n",
    "\n",
    "    # main loop to call get_all_related_pk_id() function\n",
    "    for pk_id,row in clusters.iterrows():\n",
    "        # if current pk_id has been assigned a cluster_id, it means it has already went through the recursion so no need to do it again\n",
    "        if row.cluster_id != 0:\n",
    "            continue\n",
    "        # init a blank list\n",
    "        pk_id_list = []\n",
    "        # call function\n",
    "        pk_id_list = get_all_related_pk_id(pk_id,pk_id_list)\n",
    "        # increment current max_cluster_id and assign it to new clusters\n",
    "        max_cluster_id += 1\n",
    "        clusters.loc[pk_id_list] = max_cluster_id\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = generate_clusters()\n",
    "clusters.to_csv(\"clusters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Specific cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_2_1():\n",
    "    print (\"------------------- 2.1 -------------------\")\n",
    "    print (\"How big is the cluster that contains public key (pk_id) 41442?\")\n",
    "    cluster_id_41442 = clusters.loc[41442].cluster_id\n",
    "    pk_id_in_cluster = clusters.loc[clusters['cluster_id']==cluster_id_41442].index\n",
    "    print (clusters['cluster_id'].value_counts()[cluster_id_41442])\n",
    "    print (\"Identify the cluster according to its keys with the lowest and highest numeric values.\")\n",
    "    print (\"lowest pk_id:\", pk_id_in_cluster.min())\n",
    "    print (\"highest pk_id:\", pk_id_in_cluster.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 2.1 -------------------\n",
      "How big is the cluster that contains public key (pk_id) 41442?\n",
      "50\n",
      "Identify the cluster according to its keys with the lowest and highest numeric values.\n",
      "lowest pk_id: 40284\n",
      "highest pk_id: 41911\n"
     ]
    }
   ],
   "source": [
    "answer_2_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Biggest cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_2_2():\n",
    "    print (\"------------------- 2.2 -------------------\")\n",
    "    print (\"Which cluster has the largest number of keys, and how many keys does it contain?\")\n",
    "    temp = clusters\n",
    "    cluster_id, number_of_keys = temp['cluster_id'].value_counts().idxmax(), temp['cluster_id'].value_counts().max()\n",
    "    print (\"number of keys:\",number_of_keys)\n",
    "    print (\"Identify it according to its keys with the lowest and highest numeric values.\")\n",
    "    print (\"lowest pk_id:\",temp.loc[temp['cluster_id']==cluster_id].index.min())\n",
    "    print (\"highest pk_id:\",temp.loc[temp['cluster_id']==cluster_id].index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 2.2 -------------------\n",
      "Which cluster has the largest number of keys, and how many keys does it contain?\n",
      "number of keys: 921\n",
      "Identify it according to its keys with the lowest and highest numeric values.\n",
      "lowest pk_id: 29823\n",
      "highest pk_id: 173091\n"
     ]
    }
   ],
   "source": [
    "answer_2_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Richest cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_2_3():\n",
    "    print (\"------------------- 2.3 -------------------\")\n",
    "    print (\"As of the last block in the dataset, which cluster controls the most unspent bitcoins, and how many bitcoins does it control? Again, identify it according to its keys with the lowest and highest numeric values.\")\n",
    "    pk_id_grouped_UTXOs_dict = UTXOs.groupby('pk_id')['value'].sum()\n",
    "    pk_id_grouped_UTXOs = pandas.DataFrame({'pk_id':pk_id_grouped_UTXOs_dict.index,'value':pk_id_grouped_UTXOs_dict.values}).set_index('pk_id')\n",
    "    clusters_UTXOs = pandas.merge(clusters,pk_id_grouped_UTXOs,left_index=True,right_index=True,how='inner')\n",
    "    clusters_UTXOs = clusters_UTXOs.groupby('cluster_id')['value'].sum()\n",
    "    print (\"highest unspent bitcoin:\",clusters_UTXOs.max())\n",
    "    cluster_id = clusters_UTXOs.idxmax()\n",
    "    print (\"lowest pk_id:\",clusters.loc[clusters['cluster_id']==cluster_id].index.min())\n",
    "    print (\"highest pk_id:\",clusters.loc[clusters['cluster_id']==cluster_id].index.max())\n",
    "    print (\"Which transaction is responsible for sending the largest number of bitcoins to this entity (i.e., to one or more of the keys in this cluster)?\")\n",
    "    cluster_with_highest_UTXO = clusters.loc[clusters['cluster_id']==cluster_id].index\n",
    "    output_to_this_cluster = outputs.loc[outputs['pk_id'].isin(cluster_with_highest_UTXO)]\n",
    "    tx = output_to_this_cluster.groupby('tx_id').value.sum()\n",
    "    print (\"tx_id:\",tx.idxmax(),\"value\",tx.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 2.3 -------------------\n",
      "As of the last block in the dataset, which cluster controls the most unspent bitcoins, and how many bitcoins does it control? Again, identify it according to its keys with the lowest and highest numeric values.\n",
      "highest unspent bitcoin: 4755624000000\n",
      "lowest pk_id: 37214\n",
      "highest pk_id: 39508\n",
      "Which transaction is responsible for sending the largest number of bitcoins to this entity (i.e., to one or more of the keys in this cluster)?\n",
      "tx_id: 38632 value 1186780000000\n"
     ]
    }
   ],
   "source": [
    "answer_2_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to find false positive\n",
    "def find_false_positive(temp_inputs):\n",
    "    max_cluster_id = 0\n",
    "    temp_clusters = pandas.DataFrame()\n",
    "    temp_clusters['pk_id'] = temp_inputs['sig_id'].unique()\n",
    "    temp_clusters = temp_clusters.set_index('pk_id')\n",
    "    temp_clusters['cluster_id'] = 0\n",
    "\n",
    "    def get_all_related_pk_id(pk_id,pk_id_list):\n",
    "        tx_id_array = temp_inputs.loc[temp_inputs['sig_id']==pk_id,'tx_id'].unique()\n",
    "        if temp_clusters.loc[pk_id].cluster_id != 0:\n",
    "            print(123456)\n",
    "        for tx_id_related in tx_id_array:\n",
    "            pk_id_array = temp_inputs.loc[temp_inputs['tx_id']==tx_id_related,'sig_id'].unique()\n",
    "            for pk_id_related in pk_id_array:\n",
    "                if pk_id_related not in pk_id_list:\n",
    "                    pk_id_list += [pk_id_related]\n",
    "                    if pk_id_related == pk_id:\n",
    "                        continue\n",
    "                    pk_id_list = get_all_related_pk_id(pk_id_related,pk_id_list)\n",
    "        return pk_id_list\n",
    "\n",
    "    for pk_id in temp_clusters.index:\n",
    "        if temp_clusters.loc[pk_id].cluster_id != 0:\n",
    "            continue\n",
    "        pk_id_list = []\n",
    "        pk_id_list = get_all_related_pk_id(pk_id,pk_id_list)\n",
    "        max_cluster_id += 1\n",
    "        temp_clusters.loc[pk_id_list] = max_cluster_id\n",
    "    return temp_clusters\n",
    "\n",
    "# function used to find false negative\n",
    "def find_false_negative():\n",
    "    output_pk_id_array = outputs['pk_id'].unique()\n",
    "    for pk_id in output_pk_id_array:\n",
    "        if len(outputs.loc[outputs['pk_id']==pk_id]) == 1 and len(inputs.loc[inputs['sig_id']==pk_id]) == 1:\n",
    "            tid1 = (outputs.loc[outputs['pk_id']==pk_id])['tx_id'].unique()[0]\n",
    "            cid = clusters.loc[inputs.loc[inputs['tx_id']==tid1,'sig_id'].unique()[0]].cluster_id\n",
    "            tid2 = inputs.loc[inputs['sig_id']==pk_id,'tx_id'].unique()[0]\n",
    "            if clusters.loc[pk_id].cluster_id != cid and cid != 1:\n",
    "                if len(outputs.loc[outputs['tx_id']==tid1,'pk_id']) == 2 and len(outputs.loc[outputs['tx_id']==tid2,'pk_id']) == 2 and len(inputs.loc[inputs['tx_id']==tid2,'sig_id']) == 1 and len(inputs.loc[inputs['tx_id']==tid1,'sig_id']) > 100:\n",
    "                    print (clusters.loc[pk_id].cluster_id,cid,pk_id,tid1,tid2,len(inputs.loc[inputs['tx_id']==tid1,'sig_id']))\n",
    "\n",
    "def answer_2_4():\n",
    "    print (\"------------------- 2.4 -------------------\")\n",
    "    print (\"Identify at least one potential source of false positives (keys that are clustered together but are not actually owned by the same entity) and one source of false negatives (keys that were not clustered together but are owned by the same entity).\")\n",
    "    # false positive (only list one result found by find_false_positive())\n",
    "    target_cid = 30455\n",
    "    target_tid = 119993\n",
    "    pk_id_array = clusters.loc[clusters['cluster_id']==target_cid].index\n",
    "    related_inputs = inputs.loc[inputs['sig_id'].isin(pk_id_array)]\n",
    "    tx_removed_target_tid = related_inputs.loc[related_inputs['tx_id']!=target_tid]\n",
    "    cluster_after_remove = find_false_positive(tx_removed_target_tid)\n",
    "    print (\"\\nFalse Positive:\")\n",
    "    print (\"After remove transaction with tx_id 119993, original cluster with highest pk_id:\",clusters.loc[clusters['cluster_id']==target_cid].index.max(),\"and lowest pk_id:\",clusters.loc[clusters['cluster_id']==target_cid].index.min(),\"(total number of keys: \"+str(len(clusters.loc[clusters['cluster_id']==target_cid].index))+\") would be clustered as two large group and within that number of keys would be\",cluster_after_remove['cluster_id'].value_counts().values,\".\")\n",
    "    print (\"i.e. transaction 119993 was trying to merge two large cluster, and this 'merging' transaction only happened once.\")\n",
    "    print (\"The input entries in transaction 119993:\")\n",
    "    print (related_inputs.loc[related_inputs['tx_id']==target_tid].set_index('id'))\n",
    "    # false negative (only list one result found by find_false_negative())\n",
    "    print (\"\\nFalse Negative:\")\n",
    "    target_cid1 = 870\n",
    "    target_cid2 = 89\n",
    "    target_pkid = 28601\n",
    "    target_tid1 = 28702\n",
    "    target_tid2 = 52070\n",
    "    print (\"In transaction \"+str(target_tid1)+\", there are 168 inputs and 2 outputs. It seems like one entity collects his owned bitcoins together to pay for one recipient and one collection (change) address that he controled.\")\n",
    "    print (\"The change address (pk_id: \"+str(target_pkid)+\") only happened twice: one in the output of transaction \"+str(target_tid1)+\" for collection (change), another one for spending as a input in transaction \"+str(target_tid2)+\".\")\n",
    "    print (\"The change address should be clustered same as inputs since they are all controled by same entity. But in multi-input clustering, this public key would be clustered as a distinct group (cluster_id: \"+str(target_cid1)+\") from the large entity (cluster_id: \"+str(target_cid2)+\") because it only happened one time as the input alone.\")\n",
    "    print (\"(cluster_id can be seen in the generated clusters.csv file)\")\n",
    "    print (\"\\nWhat strategy could you use to make your clustering heuristic more accurate?\")\n",
    "    print (\"1. Take extra care about some transactions happened less time but \\\"trying\\\" to merge two large clusters. (e.g. false positive above)\")\n",
    "    print (\"2. Take consideration about the output of one transaction, the change address in output should be clustered to the same group with the input. (e.g. false negative above)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 2.4 -------------------\n",
      "Identify at least one potential source of false positives (keys that are clustered together but are not actually owned by the same entity) and one source of false negatives (keys that were not clustered together but are owned by the same entity).\n",
      "\n",
      "False Positive:\n",
      "After remove transaction with tx_id 119993, original cluster with highest pk_id: 137631 and lowest pk_id: 115996 (total number of keys: 338) would be clustered as two large group and within that number of keys would be [249  89] .\n",
      "i.e. transaction 119993 was trying to merge two large cluster, and this 'merging' transaction only happened once.\n",
      "The input entries in transaction 119993:\n",
      "         tx_id  sig_id  output_id\n",
      "id                               \n",
      "163478  119993  117374     142403\n",
      "163479  119993  117354     142382\n",
      "\n",
      "False Negative:\n",
      "In transaction 28702, there are 168 inputs and 2 outputs. It seems like one entity collects his owned bitcoins together to pay for one recipient and one collection (change) address that he controled.\n",
      "The change address (pk_id: 28601) only happened twice: one in the output of transaction 28702 for collection (change), another one for spending as a input in transaction 52070.\n",
      "The change address should be clustered same as inputs since they are all controled by same entity. But in multi-input clustering, this public key would be clustered as a distinct group (cluster_id: 870) from the large entity (cluster_id: 89) because it only happened one time as the input alone.\n",
      "(cluster_id can be seen in the generated clusters.csv file)\n",
      "\n",
      "What strategy could you use to make your clustering heuristic more accurate?\n",
      "1. Take extra care about some transactions happened less time but \"trying\" to merge two large clusters. (e.g. false positive above)\n",
      "2. Take consideration about the output of one transaction, the change address in output should be clustered to the same group with the input. (e.g. false negative above)\n"
     ]
    }
   ],
   "source": [
    "answer_2_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Tagging and tracking\n",
    "### associate both individual keys and larger clusters with real entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tags():\n",
    "    tags = pandas.read_csv('tags.csv')\n",
    "    pk_id_grouped_UTXOs_dict = UTXOs.groupby('pk_id')['value'].sum()\n",
    "    pk_id_grouped_UTXOs = pandas.DataFrame({'pk_id':pk_id_grouped_UTXOs_dict.index,'value':pk_id_grouped_UTXOs_dict.values}).set_index('pk_id')\n",
    "    clusters_UTXOs = pandas.merge(clusters,pk_id_grouped_UTXOs,left_index=True,right_index=True,how='inner')\n",
    "    clusters_UTXOs = clusters_UTXOs.groupby('cluster_id')['value'].sum()\n",
    "    clusters_UTXOs = pandas.DataFrame({'cluster_id':clusters_UTXOs.index,'UTXO_value':clusters_UTXOs.values})\n",
    "    tags = pandas.merge(tags,clusters,right_index = True,left_on='pk_id',how='inner')\n",
    "    tags = pandas.merge(tags,clusters_UTXOs,on='cluster_id',how='left')\n",
    "    tags = tags.fillna(0)\n",
    "    return tags\n",
    "# columns = [type, name, pk_id, cluster_id, UTXO_value]\n",
    "tags = get_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Richest service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_3_1():\n",
    "    print (\"------------------- 3.1 -------------------\")\n",
    "    print (\"Which tagged entity controls the most unspent bitcoins, and how many bitcoins does it control? Be careful to consider entities that may control multiple tagged clusters.\")\n",
    "    temp_tags = tags.groupby('name')['UTXO_value'].sum()\n",
    "    print (temp_tags.idxmax(),int(temp_tags.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 3.1 -------------------\n",
      "Which tagged entity controls the most unspent bitcoins, and how many bitcoins does it control? Be careful to consider entities that may control multiple tagged clusters.\n",
      "PeakNevis 5185110000000\n"
     ]
    }
   ],
   "source": [
    "answer_3_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_3_2():\n",
    "    print (\"------------------- 3.2 -------------------\")\n",
    "    print (\"How many transactions sent bitcoins directly from a (fictional) exchange to a (fictional) dark market?\")\n",
    "    pk_id_exchange = clusters.loc[clusters['cluster_id'].isin(tags.loc[tags['type']=='Exchange'].cluster_id)].index\n",
    "    pk_id_darkmarket = clusters.loc[clusters['cluster_id'].isin(tags.loc[tags['type']=='DarkMarket'].cluster_id)].index\n",
    "    tx_from_exchange = inputs.loc[inputs['sig_id'].isin(pk_id_exchange)].tx_id.unique()\n",
    "    tx_from_exchange_to_dark_market = outputs.loc[(outputs['pk_id'].isin(pk_id_darkmarket)) & (outputs['tx_id'].isin(tx_from_exchange))]\n",
    "    print (len(tx_from_exchange_to_dark_market.tx_id.unique()))\n",
    "    print (\"How many bitcoins in total were sent across these transactions?\")\n",
    "    money_in = pandas.merge(inputs.loc[inputs['tx_id'].isin(tx_from_exchange_to_dark_market.tx_id.unique())][['tx_id','output_id']],outputs[['id','value']],left_on='output_id',right_on='id')[['tx_id','value']]\n",
    "    print (\"input total:\",money_in['value'].sum(),\"bitcoins\")\n",
    "    print (\"received by darkmarket:\",tx_from_exchange_to_dark_market.value.sum(),\"bitcoins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 3.2 -------------------\n",
      "How many transactions sent bitcoins directly from a (fictional) exchange to a (fictional) dark market?\n",
      "271\n",
      "How many bitcoins in total were sent across these transactions?\n",
      "input total: 3721195000000 bitcoins\n",
      "received by darkmarket: 3584219000000 bitcoins\n"
     ]
    }
   ],
   "source": [
    "answer_3_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tracking techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
